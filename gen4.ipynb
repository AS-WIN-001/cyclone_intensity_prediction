{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>904.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  latitude  longitude  speed\n",
       "0   0     128.0      904.0     60\n",
       "1   0     134.0      906.0     75\n",
       "2   0     140.0      907.0     85\n",
       "3   0     145.0      908.0     65\n",
       "4   0     150.0      909.0     60"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('temp_data2.csv')\n",
    "df['latitude'] = df['latitude'].apply(lambda x: x[:-1])\n",
    "df['longitude'] = df['longitude'].apply(lambda x: x[:-1])\n",
    "df[['latitude', 'longitude']] = df[['latitude', 'longitude']].astype(float)\n",
    "date_time = pd.to_datetime(df.pop('time'), format='%Y%m%d%H')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_id_ds = df.id.unique()\n",
    "np.random.shuffle(cyc_id_ds)\n",
    "n = len(cyc_id_ds)\n",
    "train_ids = cyc_id_ds[0:int(n*0.7)]\n",
    "val_ids = cyc_id_ds[int(n*0.7):int(n*0.9)]\n",
    "test_ids = cyc_id_ds[int(n*0.9):]\n",
    "\n",
    "train_df = df[0:int(n*0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_collection = []\n",
    "test_df_collection = []\n",
    "val_df_collection = []\n",
    "\n",
    "cid = df.pop('id')\n",
    "MEAN = df.mean()\n",
    "STD = df.std()\n",
    "df = (df - MEAN) / STD\n",
    "df = df.assign(id=cid)\n",
    "\n",
    "for _df in [df.loc[df['id'] == id] for id in train_ids]:\n",
    "    _df.pop('id')\n",
    "    train_df_collection.append(_df)\n",
    "\n",
    "for _df in [df.loc[df['id'] == id] for id in test_ids]:\n",
    "    _df.pop('id')\n",
    "    test_df_collection.append(_df)\n",
    "\n",
    "for _df in [df.loc[df['id'] == id] for id in val_ids]:\n",
    "    _df.pop('id')\n",
    "    val_df_collection.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude      48.353828\n",
      "longitude    122.472912\n",
      "speed         22.389924\n",
      "dtype: float64\n",
      "latitude     140.162766\n",
      "longitude    784.186350\n",
      "speed         40.016973\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(STD)\n",
    "print(MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width,\n",
    "        label_width,\n",
    "        shift,\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        label_columns=None,\n",
    "    ):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {\n",
    "                name: i for i, name in enumerate(label_columns)\n",
    "            }\n",
    "\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"Total window size: {self.total_window_size}\",\n",
    "                f\"Input indices: {self.input_indices}\",\n",
    "                f\"Label indices: {self.label_indices}\",\n",
    "                f\"Label column name(s): {self.label_columns}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [\n",
    "                    labels[:, :, self.column_indices[name]]\n",
    "                    for name in self.label_columns\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "        )\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_timeseries_dataset(input_width, label_width, shift):\n",
    "    train_ds = None\n",
    "    for tdf in train_df_collection:\n",
    "        w = WindowGenerator(input_width=input_width, label_width=label_width, shift=shift, train_df=tdf, val_df=None, test_df=None, label_columns=['speed'])\n",
    "        if train_ds == None:\n",
    "            train_ds = w.train\n",
    "        else:\n",
    "            train_ds = train_ds.concatenate(w.train)    \n",
    "\n",
    "    val_ds = None\n",
    "    for tdf in val_df_collection:\n",
    "        w = WindowGenerator(input_width=input_width, label_width=label_width, shift=shift, train_df=tdf, val_df=None, test_df=None, label_columns=['speed'])\n",
    "        if val_ds == None:\n",
    "            val_ds = w.train\n",
    "        else:\n",
    "            val_ds = val_ds.concatenate(\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "w.train)\n",
    "\n",
    "    test_ds = None\n",
    "    for tdf in test_df_collection:\n",
    "        w = WindowGenerator(input_width=input_width, label_width=label_width, shift=shift, train_df=tdf, val_df=None, test_df=None, label_columns=['speed'])\n",
    "        if test_ds == None:\n",
    "            test_ds = w.train\n",
    "        else:\n",
    "            test_ds = test_ds.concatenate(w.train)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = create_tensor_timeseries_dataset(4, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 18ms/step - loss: 0.4016 - mean_absolute_error: 0.3931 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "baseline = Baseline(label_index=2)\n",
    "\n",
    "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanAbsoluteError(), 'accuracy'])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "prediction_error = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(val_ds)\n",
    "performance['Baseline'] = baseline.evaluate(test_ds, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 10\n",
    "\n",
    "\n",
    "def compile_and_fit(model, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=patience, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.RootMeanSquaredError(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS, validation_data=val_ds, callbacks=[early_stopping]\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def get_prediction_error(model, test_ds):\n",
    "    count = 0\n",
    "    diff = 0\n",
    "    for i in test_ds.as_numpy_iterator():\n",
    "        input, label = i\n",
    "        prediction = np.array(model(input))\n",
    "\n",
    "        for i, p in enumerate(prediction):\n",
    "            p = p * STD[\"speed\"] + MEAN[\"speed\"]\n",
    "            a = label[i] * STD[\"speed\"] + MEAN[\"speed\"]\n",
    "            diff += np.abs(a - p)\n",
    "            count += 1\n",
    "    \n",
    "    return diff/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 4.8953 - mean_absolute_error: 1.6334 - root_mean_squared_error: 2.2125 - val_loss: 4.4842 - val_mean_absolute_error: 1.5383 - val_root_mean_squared_error: 2.1176\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 3.0860 - mean_absolute_error: 1.2829 - root_mean_squared_error: 1.7567 - val_loss: 2.9196 - val_mean_absolute_error: 1.2287 - val_root_mean_squared_error: 1.7087\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 1.9569 - mean_absolute_error: 1.0140 - root_mean_squared_error: 1.3989 - val_loss: 1.9209 - val_mean_absolute_error: 0.9888 - val_root_mean_squared_error: 1.3860\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 1.2668 - mean_absolute_error: 0.8139 - root_mean_squared_error: 1.1255 - val_loss: 1.2893 - val_mean_absolute_error: 0.8109 - val_root_mean_squared_error: 1.1355\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.8518 - mean_absolute_error: 0.6706 - root_mean_squared_error: 0.9230 - val_loss: 0.8968 - val_mean_absolute_error: 0.6802 - val_root_mean_squared_error: 0.9470\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.6076 - mean_absolute_error: 0.5703 - root_mean_squared_error: 0.7795 - val_loss: 0.6592 - val_mean_absolute_error: 0.5840 - val_root_mean_squared_error: 0.8119\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.4673 - mean_absolute_error: 0.5010 - root_mean_squared_error: 0.6836 - val_loss: 0.5201 - val_mean_absolute_error: 0.5155 - val_root_mean_squared_error: 0.7211\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.3889 - mean_absolute_error: 0.4530 - root_mean_squared_error: 0.6236 - val_loss: 0.4420 - val_mean_absolute_error: 0.4684 - val_root_mean_squared_error: 0.6648\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.3467 - mean_absolute_error: 0.4214 - root_mean_squared_error: 0.5888 - val_loss: 0.4004 - val_mean_absolute_error: 0.4377 - val_root_mean_squared_error: 0.6328\n",
      "Epoch 10/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.3251 - mean_absolute_error: 0.4023 - root_mean_squared_error: 0.5702 - val_loss: 0.3794 - val_mean_absolute_error: 0.4185 - val_root_mean_squared_error: 0.6160\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3794 - mean_absolute_error: 0.4185 - root_mean_squared_error: 0.6160\n"
     ]
    }
   ],
   "source": [
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=4),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(linear)\n",
    "\n",
    "val_performance['Linear'] = linear.evaluate(val_ds)\n",
    "performance['Linear'] = linear.evaluate(test_ds, verbose=0)\n",
    "prediction_error['Linear'] = get_prediction_error(linear, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6550 - mean_absolute_error: 0.5440 - root_mean_squared_error: 0.8093 - val_loss: 0.4218 - val_mean_absolute_error: 0.4401 - val_root_mean_squared_error: 0.6495\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.3133 - mean_absolute_error: 0.3846 - root_mean_squared_error: 0.5598 - val_loss: 0.3564 - val_mean_absolute_error: 0.3987 - val_root_mean_squared_error: 0.5970\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.2984 - mean_absolute_error: 0.3750 - root_mean_squared_error: 0.5463 - val_loss: 0.3517 - val_mean_absolute_error: 0.3957 - val_root_mean_squared_error: 0.5930\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.2943 - mean_absolute_error: 0.3727 - root_mean_squared_error: 0.5425 - val_loss: 0.3485 - val_mean_absolute_error: 0.3934 - val_root_mean_squared_error: 0.5904\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.2916 - mean_absolute_error: 0.3710 - root_mean_squared_error: 0.5400 - val_loss: 0.3467 - val_mean_absolute_error: 0.3923 - val_root_mean_squared_error: 0.5888\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.2896 - mean_absolute_error: 0.3702 - root_mean_squared_error: 0.5382 - val_loss: 0.3453 - val_mean_absolute_error: 0.3915 - val_root_mean_squared_error: 0.5876\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.2880 - mean_absolute_error: 0.3693 - root_mean_squared_error: 0.5367 - val_loss: 0.3439 - val_mean_absolute_error: 0.3903 - val_root_mean_squared_error: 0.5865\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.2867 - mean_absolute_error: 0.3686 - root_mean_squared_error: 0.5354 - val_loss: 0.3435 - val_mean_absolute_error: 0.3898 - val_root_mean_squared_error: 0.5861\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.2853 - mean_absolute_error: 0.3678 - root_mean_squared_error: 0.5341 - val_loss: 0.3425 - val_mean_absolute_error: 0.3888 - val_root_mean_squared_error: 0.5852\n",
      "Epoch 10/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.2841 - mean_absolute_error: 0.3672 - root_mean_squared_error: 0.5330 - val_loss: 0.3420 - val_mean_absolute_error: 0.3881 - val_root_mean_squared_error: 0.5848\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3420 - mean_absolute_error: 0.3881 - root_mean_squared_error: 0.5848\n"
     ]
    }
   ],
   "source": [
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(dense)\n",
    "\n",
    "val_performance['Dense'] = dense.evaluate(val_ds)\n",
    "performance['Dense'] = dense.evaluate(test_ds, verbose=0)\n",
    "prediction_error['Dense'] = get_prediction_error(dense, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.6499 - mean_absolute_error: 0.5007 - root_mean_squared_error: 0.8061 - val_loss: 0.2465 - val_mean_absolute_error: 0.3367 - val_root_mean_squared_error: 0.4965\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.1429 - mean_absolute_error: 0.2677 - root_mean_squared_error: 0.3781 - val_loss: 0.0913 - val_mean_absolute_error: 0.2110 - val_root_mean_squared_error: 0.3022\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0844 - mean_absolute_error: 0.2032 - root_mean_squared_error: 0.2906 - val_loss: 0.0703 - val_mean_absolute_error: 0.1858 - val_root_mean_squared_error: 0.2652\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 4s 20ms/step - loss: 0.0731 - mean_absolute_error: 0.1863 - root_mean_squared_error: 0.2704 - val_loss: 0.0631 - val_mean_absolute_error: 0.1767 - val_root_mean_squared_error: 0.2512\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0680 - mean_absolute_error: 0.1793 - root_mean_squared_error: 0.2607 - val_loss: 0.0614 - val_mean_absolute_error: 0.1747 - val_root_mean_squared_error: 0.2478\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0646 - mean_absolute_error: 0.1752 - root_mean_squared_error: 0.2542 - val_loss: 0.0601 - val_mean_absolute_error: 0.1729 - val_root_mean_squared_error: 0.2451\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0622 - mean_absolute_error: 0.1720 - root_mean_squared_error: 0.2494 - val_loss: 0.0588 - val_mean_absolute_error: 0.1715 - val_root_mean_squared_error: 0.2425\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0602 - mean_absolute_error: 0.1696 - root_mean_squared_error: 0.2454 - val_loss: 0.0588 - val_mean_absolute_error: 0.1719 - val_root_mean_squared_error: 0.2425\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 3s 20ms/step - loss: 0.0586 - mean_absolute_error: 0.1672 - root_mean_squared_error: 0.2420 - val_loss: 0.0589 - val_mean_absolute_error: 0.1716 - val_root_mean_squared_error: 0.2427\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.0589 - mean_absolute_error: 0.1716 - root_mean_squared_error: 0.2427\n"
     ]
    }
   ],
   "source": [
    "CONV_WIDTH = 4\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "history = compile_and_fit(conv_model)\n",
    "\n",
    "val_performance['Conv'] = conv_model.evaluate(val_ds)\n",
    "performance['Conv'] = conv_model.evaluate(test_ds, verbose=0)\n",
    "prediction_error['Conv'] = get_prediction_error(linear, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "174/174 [==============================] - 5s 22ms/step - loss: 1.0474 - mean_absolute_error: 0.7161 - root_mean_squared_error: 1.0234 - val_loss: 1.2516 - val_mean_absolute_error: 0.7340 - val_root_mean_squared_error: 1.1187\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.7598 - mean_absolute_error: 0.6072 - root_mean_squared_error: 0.8717 - val_loss: 0.4268 - val_mean_absolute_error: 0.4717 - val_root_mean_squared_error: 0.6533\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.2663 - mean_absolute_error: 0.3827 - root_mean_squared_error: 0.5161 - val_loss: 0.1765 - val_mean_absolute_error: 0.2949 - val_root_mean_squared_error: 0.4201\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.1360 - mean_absolute_error: 0.2638 - root_mean_squared_error: 0.3688 - val_loss: 0.1107 - val_mean_absolute_error: 0.2300 - val_root_mean_squared_error: 0.3327\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.0966 - mean_absolute_error: 0.2179 - root_mean_squared_error: 0.3109 - val_loss: 0.0826 - val_mean_absolute_error: 0.2013 - val_root_mean_squared_error: 0.2874\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.0785 - mean_absolute_error: 0.1936 - root_mean_squared_error: 0.2801 - val_loss: 0.0695 - val_mean_absolute_error: 0.1833 - val_root_mean_squared_error: 0.2637\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.0675 - mean_absolute_error: 0.1777 - root_mean_squared_error: 0.2599 - val_loss: 0.0611 - val_mean_absolute_error: 0.1716 - val_root_mean_squared_error: 0.2472\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 4s 22ms/step - loss: 0.0615 - mean_absolute_error: 0.1692 - root_mean_squared_error: 0.2481 - val_loss: 0.0565 - val_mean_absolute_error: 0.1663 - val_root_mean_squared_error: 0.2377\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.0581 - mean_absolute_error: 0.1645 - root_mean_squared_error: 0.2409 - val_loss: 0.0547 - val_mean_absolute_error: 0.1634 - val_root_mean_squared_error: 0.2339\n",
      "Epoch 10/10\n",
      "174/174 [==============================] - 4s 21ms/step - loss: 0.0562 - mean_absolute_error: 0.1618 - root_mean_squared_error: 0.2371 - val_loss: 0.0538 - val_mean_absolute_error: 0.1617 - val_root_mean_squared_error: 0.2320\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.0538 - mean_absolute_error: 0.1617 - root_mean_squared_error: 0.2320\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(10, recurrent_activation='relu', return_sequences=False),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model)\n",
    "\n",
    "val_performance['LSTM'] = lstm_model.evaluate(val_ds)\n",
    "performance['LSTM'] = lstm_model.evaluate(test_ds, verbose=0)\n",
    "prediction_error['LSTM'] = get_prediction_error(lstm_model, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cyclone_intensity_prediction.pickle','wb') as f:\n",
    "    pickle.dump(lstm_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
